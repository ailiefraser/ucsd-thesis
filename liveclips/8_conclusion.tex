\section{Limitations \& Future Work}
This chapter presented initial work exploring ways to bring inspiring examples into the creative process, and a new type of content from which to draw such examples: creative live stream videos. We provide suggestive results that our algorithm can select potentially inspiring clips, and some initial user feedback on our prototype interfaces indicating that this is a promising avenue for future work. Rather than conduct a rigorous controlled study (which is difficult and often inappropriate for evaluating goals like creativity and inspiration \cite{Shneiderman2007}), this chapter aims to introduce this space and lay out the possibilities for future work to explore. In this section, we discuss a few main directions for such work.

\subsection{More Robust Segmentation and Change Ranking}
Our current methods for detecting visual change use simple computer vision approaches, and exhibited some failure cases. We have yet to try more sophisticated deep learning methods to segment this data but this is a promising direction of future work that could help improve the classification of panning, zooming, and parameter setting. Having more robust and available usage data could also improve the detection of such actions. For example, Lafreniere \textit{et al.} \cite{Lafreniere2014} used instrumented software to gather both videos and detailed usage, which allowed them to calculate visual change directly by counting the number of pixels on the artist's document that change during a video clip. As of recently, streamers on Behance (\href{https://behance.net/live}{\nolinkurl{behance.net/live}}) can use a plugin that logs their tool use while they stream in creative software. This data can be used to segment live stream archives \cite{Fraser2020}, but such segmentations would still benefit from additional visual analysis to identify events that are not captured in usage logs and understand visual properties of the artist's work. Finally, to make use of the large amount of video content that already exists online with no available telemetry, a deep learning system could be trained on those videos that do have usage data, and then applied to videos that do not.

\subsection{Making Use of Audio and Chat Logs}
Live streamed videos come with additional data that this work did not make use of: audio in the form of the streamer's narration and responses to questions (which can be transcribed to text) and chat logs from viewers of the stream. Making use of audio narration in software tutorial videos is an open problem \cite{Chi2012} as narrations don't always align exactly with the artist's actions. Live streamed videos have a similar problem that is exacerbated by the fact that artists are not always narrating what they are doing; as our formative work found, sometimes streamers answer questions from the chat and other times they talk about unrelated things as they work. Future work should explore ways to detect when those different types of narration are occurring, and make use of their content as appropriate, for example highlighting moments where artists answer questions, or building a mapping of ways in which artists describe their software actions in natural language.

Chat logs may also be a useful data source to include in future work. Though the content of live chats tends to be very noisy, especially on popular channels \cite{Hamilton2014}, the frequency of chat posts at a given time can indicate exciting or interesting moments \cite{Pan2016}. Chat post frequency could therefore be used as another criteria for ranking clips. In addition, our formative work found that questions asked in the chat often go unanswered, usually because the streamer does not see them. Finding and highlighting such questions after the fact and encouraging the streamer (or other viewers) to answer them later could help keep viewers engaged once streams are archived (which several viewers in our formative surveys desired), as well as surface valuable feedback and ideas for the streamer.

\subsection{Other Ways to Generate Short Clips from Long Videos}
This work focused on generating tool-focused clips, inspired by prior work \cite{Grossman2010a, Lafreniere2014} and the natural mapping between tool use and user behaviour. However, while tool-focused clips are good for showing users how a tool works \cite{Grossman2010a}, they may not be the most inspirational types of clips one can generate from long videos. Other methods could include breaking videos into higher level sub-tasks (based on pauses in tool activity and narration content), or extracting clips where the artist describes a particular technique or answers a question. As one of the exciting parts of watching a live stream is seeing an artist go from a blank canvas to a finished work, another option could be to shorten the entire video down to a short summary. This could be done by removing sections where the artist takes pauses, talks with no actions, and switches applications; and by adapting existing techniques for creating short summaries from long videos (\textit{e.g.}, \cite{Truong2007}). 

\section{Conclusion}
This chapter explored a growing form of creative videos very different from traditional tutorials: live streams. We found that creative live streams are a good source of both learning and inspiration, and introduced an approach for bringing inspiration into the context of creative software users' workflows. 

RePlay, ReMap, and LiveClips all leveraged \textbf{visual media} in the form of screencast videos for providing contextual support towards understanding creative processes. But for users who just want to get a task done without worrying about the process behind it, videos can be too tedious and detailed. The next two chapters explore how two different types of resource -- \textbf{executable code} and \textbf{written text} -- can help people quickly and easily achieve creative outcomes.


\section{Acknowledgements}
We thank Tricia Ngoon, Kandarp Khandwala, and Nicolas La-polla for their help with live stream analysis, and our study participants for their insights. This work was supported in part by NSERC, Adobe Research, and NSF award \#1735234.

This chapter, in part, includes portions of material as it appears in \textit{Sharing the Studio: How Creative Livestreaming can Inspire, Educate, and Engage} by C. Ailie Fraser, Joy O. Kim, Alison Thornsberry, Scott Klemmer, and Mira Dontcheva in the Proceedings of the 2019 on Creativity and Cognition (C\&C '19). The dissertation author was the primary investigator and author of this paper.

This chapter, in part, includes portions of material coauthored with Andy Edmonds and Mira Dontcheva. The dissertation author was the primary investigator and author of this material.