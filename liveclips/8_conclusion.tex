\section{Limitations \& Future Work}
This paper presented initial work exploring ways to bring inspiring examples into the creative process, and a new type of content from which to draw such examples: creative live-stream videos. We provide suggestive results that our algorithm can select potentially inspiring clips, and some initial user feedback on our prototype interfaces indicating that this is a promising avenue for future work. Rather than conduct a rigorous controlled study (which is difficult and often inappropriate for evaluating goals like creativity and inspiration \cite{Shneiderman2007}, this paper aims to introduce this space and lay out the possibilities for future work to explore. In this section, we discuss a few main directions for such work.

\subsection{More robust segmentation and change ranking}
Our current methods for detecting visual change use similar computer vision approaches, and exhibited some failure cases. We have yet to try more sophisticated deep learning methods to segment this data but this is a promising direction of future work that could help improve classifying panning, zooming, and parameter setting. However, with more robust and available usage data, detection of such features from video would not be necessary. Creative software such as Photoshop and Illustrator could provide their own in-app live-streaming option that directly logs usage while an artist is streaming, which would make indexing and recommending videos to users much easier. For example, Lafreniere et al. \cite{Lafreniere2014} used instrumented software to gather both videos and detailed usage, which allowed them to calculate visual change directly by counting the number of pixels on the artist's document that change during a video clip.

Alternatively, to make use of the large amount of content currently online with no available telemetry, a deep learning system could be trained on those videos that do have usage data, and then applied to videos that do not.

\subsection{Making use of audio and chat logs}
Live-streamed videos come with additional data that this work did not make use of: audio in the form of the streamer's narration and responses to questions (which can be transcribed to text) and chat logs from viewers of the stream. Making use of audio narration in software tutorial videos is an open problem \cite{Chi2012} as narrations don't always align exactly with the artist's actions. Live-streamed videos have a similar problem that is exacerbated by the fact that artists are not always narrating what they are doing; sometimes they are answering questions from the chat and other times they are simply talking about unrelated things as they work. Future work should explore ways to detect when those different types of narration are occurring, and make use of their content as appropriate, for example highlighting moments where artists answer questions, or building a mapping of ways in which artists describe their software actions in natural language.

Chat logs can also be a useful data source to include in future work. Though the content of live chats tends to be very noisy, especially on popular channels \cite{Hamilton2014}, the frequency of chat posts at a given time can indicate exciting or interesting moments \cite{Pan2016}. Chat post frequency could therefore be used as another criteria for ranking clips.

\subsection{Other ways to generate short clips from long videos}
This work focused on generating tool-based clips, inspired by prior work \cite{Grossman2010a, Lafreniere2014} and the natural mapping between tool use and user behaviour. However, while tool-focused clips are good for showing users how a tool works \cite{Grossman2010a}, they may not be the most inspirational types of clips one can generate from long videos. Other methods could include breaking videos into higher level sub-tasks (based on pauses in tool activity and narration content), or extracting clips where the artist describes a particular technique or answers a question. As one of the exciting parts of watching a live-stream is seeing an artist go from a blank canvas to a finished work, another option could be to shorten the entire video down to a short summary. This could be done by removing sections where the artist takes pauses, talks with no actions, and switches applications; and by adapting existing techniques for creating short video summaries from long videos (e.g., \cite{Truong2007}). 

\subsection{Looking forward: the intersection between learning and inspiration}
As discussed earlier, creative live-streams can support both goals of learning and inspiration. In the current implementations, in-app videos are only loosely tied to their sources in the browser. Users can go from a video clip to its original video, but another interesting application could be to support the opposite direction: finding a video in the browser, and bringing it into the context of an application, guiding the user interactively through following the artist's actions or otherwise augmenting the interface with clips from the video at relevant moments. This is one potential way to combine the benefits of learning and inspiration; there are surely many more yet to explore.
