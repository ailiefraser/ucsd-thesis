\subsubsection{Extracting clips}
The first step toward recommending short tool-based clips is extracting candidate clips from long videos. Our heuristics for this are based on those proposed by Lafreniere \textit{et al.} [investigating]: for a given software tool, we first group all consecutive invocations of the tool (within 5 seconds of each other). If the tool was selected less than 10 seconds before the first invocation, we include the selection in the group. We add 2 seconds of padding to the beginning so that the user can see the action of the tool being first selected or invoked. We then trim clip length to a max of 25 seconds, or if it is shorter, we extend it to 25 (maybe 15?) seconds, as prior user studies have indicated that 15-25 seconds is a desirable length for in-app video clip recommendations.

We also ignore tool events that are for navigation (\textit{e.g.}, zoom, scroll, select) as these tend to happen frequently in visual software throughout a given task. We also ignore events immediately followed by undo, or events where a different tool was used within three seconds of the current tool being used (as sometimes artists will select or invoke a tool accidentally). We shorten clips where the context likely changed before it was over, i.e. if a document is closed or opened. If a clip ends up shorter than 15 seconds, we remove it [lafreniere].

Lafreniere \textit{et al.}'s method is aimed at creating instructional clips showing how to use a tool, whereas LiveClips is aimed at creating inspiring clips. Therefore, the method described above may not necessarily be the best method for LiveClips. As an alternative, we also create clips where instead of shortening tool use to 25 seconds when it is longer, we speed up the entire section so it fits into 25 seconds. We refer to these as timelapse videos. Since artists who are not focused on making tutorials may use the same tool for a long period of time (\textit{e.g.} a brush tool to create artwork), the incremental progress they make in the first 25 seconds may be less interesting than the overall progress they make while using that tool. Other alternative methods could be used as well, such as taking the middle 25-seconds of a long segment, or creating a slideshow of screenshots from throughout the video. This work focused on comparing these two methods as promising starting points.

In our sample set of X Y-hour-long videos, this produced Z clips. This is comparable (maybe) with Lafreniere \textit{et al.} [], who generated approximately 2500 clips from 25.5 hours of footage.

\subsection{Detecting and masking unwanted change}
Ideally we only want to detect visual change that happens on the canvas, because this is the inspiring part. However, there many other types of visual change that happen in these videos, such as switching windows, opening dialogs, and zooming in and out. Some of these changes (\textit{e.g.} zooming) are simply uninteresting, and others (\textit{e.g.} setting parameters in a dialog) may be helpful in a tutorial but are less interesting for inspiration. The main goal for this content is to learn what possibilities there are, not learn specifics about how to do something. Though users might want to see more specifics later if they want to try and follow the artist's method, they will only reach this point if they find the content inspiring in the first place. 

LiveClips's current implementation detects and excludes motion caused by navigational movement (scrolling and zooming), changing application windows, and the streamer's face movement. This leaves canvas changes and other UI changes. Although we are primarily interested in canvas changes, we leave the task of separating these from UI changes to future work. The parameters for the methods described below were tweaked through trial and error until they were effective on our sample set of data, which we believe is reasonably representative of live streamed creative software use. The future work section describes additional factors that could be used to improve the ranking of clips beyond visual change only.

\subsubsection{Detecting the artist's face}
Most live streamed videos show the artist's full screen with a small webcam view of their face inserted somewhere, usually the bottom left corner. LiveClips aims to mask out this area in each clip before detecting other changes, as we do not want to include motion caused by the artist moving. In this work, we assume that the faces to ignore will be in one of the two bottom corners (within 150 pixels of the edges in the scaled-down version of the video which is approximately 360x640 pixels). This assumption could be loosened to anywhere around the edges, but one must be careful as paintings or photos of people that appear in the artists' work can also get captured by face detection. All the videos in our dataset had the artist in the bottom left corner. We also assume that the artist stays in the same place throughout the video, which is true for all the videos in our dataset.

For each video, we run face detection on the first clip, implemented using the Viola-Jones algorithm in \textsc{matlab}. We look at every 5th frame only (for efficiency) and find all faces that appear in at least 5 different frames in the clip (so that one-off outliers do not get captured) and are located within 150px of the bottom of the frame and either the right or left edge of the frame. We consider two faces from two different frames the ``same'' if they are located within a 10px radius of each other (since artists tend to move around a little but not too much). 

We then do the same for every 5th clip, and only save a face location if it is found in at least 2 different clips (to avoid stray faces appearing in individual clips only). Once the set of saved faces stops growing, we stop looking at more clips. This meant that the algorithm usually only looked at 2-4 clips for each video. Given this set of saved face locations, we then mask out the corner in which they appear with a buffer size of the detected face's dimensions.

We note that this could be done more robustly by running face detection on all clips, but this takes a long time to process and we deemed it unnecessary, as there tends not to be much change in people's faces or locations over the course of a multi-hour video. Our method has the following limitations: If the person is not located around one of the bottom corners of the screen they will not get masked, and if the person has a picture of a face around a bottom corner of the screen it will probably get masked. However, if such a face does exist it is unlikely to change much (the artist may be leaving it there as a reference image), so excluding it from change calculations is acceptable.

\subsubsection{Detecting document and application changes}
LiveClips detects document changes (\textit{e.g.}, a document being opened or closed) from the usage data, as most creative software logs these events in addition to tool events. 

LiveClips detects application window changes using a simple computer vision approach. First, if more than 90\% of the pixels change between one frame and the next, the streamer likely switched windows, so LiveClips ends the clip one second before this change. As a second check, for each full video LiveClips saves the top 50 pixels as a template to match all other future clips against. Then for each frame of each clip, LiveClips calculates the average change between the top 50 pixels of the frame and the template, and if this number is more than 25, ends the clip one second before. This is to catch changes such as going from Illustrator to Photoshop, as these applications look rather similar and so it is unlikely that 90\% of the pixels change, but the top menu bar changes for every application.

\subsubsection{Detecting navigation changes}
LiveClips uses a mix of usage data and computer vision, as some zoom and pan events are logged in the usage data but not all (depending on how the user invoked them).

For all navigation events in usage logs, we exclude all frames that happen between that event and the next event from future change calculations.

To detect panning motions that may not have been captured by usage logs, LiveClips does simple feature detection and point feature matching across consecutive frames. For each frame:
\begin{enumerate}
\item Detect FAST features (corners) in \textsc{matlab} on both the current frame and the previous frame
\item Extract Fast Retina Keypoint (FREAK) descriptors for these features
\item Match the features using Hamming distance to find correspondences between the points across the two frames
\item Calculate the euclidean distance between each pair of matched points and store them in a vector. If a pair of matched points has distance zero, this implies that feature stayed in the same place between the two frames. Otherwise, this implies the feature has moved to a new location.
\item Count the number of nonzero values in this vector. If there are more than 15, there was likely some motion or zooming, so we exclude this frame and the one before and after it (i.e. set all their pixels to 0 so no change will be detected). This simple feature matching is not perfect, so there are usually a few outlier matches. This threshold of 15 helps to prevent those outliers from getting captured.
\end{enumerate}

This approach has the following limitations: If the content of the artist's canvas is low contrast or does not have many corners, motion may not be detected because no or few feature points will be detected there. If the artist moves something large on the canvas but does not actually move the canvas, motion probably will be detected when it shouldn't. However this is not a crucial limitation, as moving an object from one place to another is likely not a very interesting change anyway.

\subsection{Measuring visual change}
After all the previously mentioned types of motion have been excluded, we detect how much visual change happens in each clip to determine the clip's rank. For each frame, we calculate the difference between that frame and the previous frame. This outputs a video-clip showing only the pixels that are changed every frame. We average this clip across all 3 dimensions (x, y, time) to obtain a numeric value representing the average amount of change in that clip. To see whether this method is reasonably accurate, we can average across time only to produce an image where the brightness of a pixel indicates how much change happens at that pixel throughout the clip (figure, probably).
