\section{Discussion}

\subsection{Can We Really Predict What Will Be Inspiring?}
The results from the ranking evaluation presented in the previous section should be taken with a grain of salt. The workers who participated in this study were not all users of creative software, and the video clips were presented out of context. The evaluation was intended as an initial baseline to determine whether LiveClips' ranking algorithm can reasonably identify good clips. The subjective nature of the question workers were asked meant that we could not include a ``ground truth'' comparison to filter out lazy turkers. We did however enforce that the entire video clips played at least once before workers could select an answer.

Aside from the questions that using crowd-worker participants raise, the idea of ``inspiration'' is subjective and hard to predict. What one person finds inspiring another may not. Whether someone finds something inspiring could even change depending on the time at which they see it. Despite these challenges, we had reasonable agreement among workers overall; the average percent of agreement across all pairs of clips was 74\% (SD 14.5). Even if Mechanical Turk workers \textit{were} a 100\% reliable source, we would still expect some disagreement, due to the subjective nature of the question.

The LiveClips algorithm can reasonably identify clips that most workers agree are either good or bad. It is important to keep in mind that the main purpose for this algorithm is to address the ``cold start'' problem. Once people start using an interface with video examples, LiveClips could continually improve the ranking algorithm based on user behaviour and preferences, exhibited through how much people interact with the videos. %There are other low-level features of clips identified by Lafreniere \textit{et al.} \cite{Lafreniere2014} that could also be included in a ranking algorithm, such as whether a clip shows parameters being set, but we chose not to include these in the LiveClips algorithm because it is likely that the \textit{content} of a video's artwork will be the main factor that determines its inspirational value. but this is a hard problem.

\subsection{How Diverse Should the Set of Examples Be?}
Research on examples and creativity is rather divided regarding whether diverse examples that are distantly related to the user's task are better than a narrow set of examples that are more closely related to the user's task. Some work has shown that more diverse, far-off examples improve creativity by encouraging people to think more broadly and try new things \cite{Chan2011, Siangliulue2015a}, while other work has shown that similar examples are more useful as they are more relevant to the user \cite{Chan2015}. More recently, Benjamin \textit{et al.} \cite{Benjamin2014} propose letting the user decide by providing an adjustable slider that determines the diversity of recommended examples, based on the idea that the need for more diverse or more similar examples may differ depending on where the user is in their process.

Another option could be to let users search by example, a feature that is now common in search engines. Rather than having to specify an exact query, users could provide an example video and ask for ``more like this''. However even in this case the diversity of results is an important factor to consider.

%let's cut this for now.It's good but not as connected to inspiration as the the previous sections, which work well together
%\subsection{How much information should recommendations show?}
%Prior research has shown the benefits of being transparent with users about how recommendations are chosen \cite{}. In our three interfaces, contextual tooltips are transparent by nature, as they show only clips of the selected tool. However the method for choosing clips in the search window and panel is currently rather opaque. It may be helpful to provide users with more detail regarding why a clip was chosen, for example because they used a certain tool recently. 

%More generally, providing users with enough information scent to make a quick but informed decision about whether they should give their attention to an example is important. This highlights a challenging trade-off in the design of contextual interfaces: showing users enough information without making it too costly to review this information \cite{?}. While there are many additional things that could be shown along with a video clip (a description of it, the audio transcript, the tools used in it, an overview of the task it shows, etc.) and existing methods for highlighting important activity within the clip (highlighting mouse movement or areas of change \cite{}, showing what keys are being pressed when \cite{}), incorporating all of this information would quickly become overwhelming. Prior work has shown that a brief text description can help users efficiently scan a set of video clips \cite{}; this may be a good starting point.