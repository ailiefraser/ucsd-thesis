\section{Discussion: The Potential of Multimodal Help Search}
Overall, most participants were positive about multimodal help search, and many of the challenges they exhibited stemmed from either implementation issues or unfamiliarity with using speech and pointing for search and navigation. It is likely that an improved implementation combined with more time spent using ReMap would further increase users' success. This section outlines how ReMap's usability and implementation could be improved based on the study findings.

\subsection{Usability Challenges With Speech for Search}
\subsubsection{The Midas Touch Problem: How best to initiate a search?}
Since ReMap's speech detection is always on, participants occasionally encountered a ``Midas touch'' problem \cite{Jacob1990} of searching unintentionally. This is a challenge with a system that is always listening; personal assistants that can be activated via a voice command (\textit{e.g.}, ``Hey Siri'') cause similar accidents \cite{Hern2019}. On the other hand, needing to press a button to engage speech recognition can be burdensome, and in our study, accidental searches only happened 3 times out of all 121 speech queries issued. ReMap's current design (like many personal voice assistants today) was informed largely by the goal to make searching as quick and easy as possible, but for something as intentional as a search query, the added burden of pressing a button may still be more convenient than the risk of error. Future work should compare ReMap's current design of speaking a keyword to search with a button or keyboard shortcut to activate speech detection. 

\subsubsection{Pre-emptive searching: How to indicate a query is finished?}
Several participants were frustrated by ReMap issuing a search before they had finished speaking their query, usually because they had paused briefly to think about what to say next and the Web Speech \textsc{api} interpreted this as the end of a phrase. Most voice assistants have this same functionality of automatically detecting when the user is finished speaking (\textit{e.g.}, Siri, Alexa), but they do sometimes fail and cut the user off early. A study by Jiang et al. \cite{Jiang2013} found that these ``system interruptions'' accounted for about 10\% of all transcription errors. In our study, they accounted for about 24\% (7/29) of all transcription errors, including speech recognition errors and unintentional searches. Similar to the alternatives proposed above, future work should explore whether it is preferable to require the user to explicitly press a button, say a keyword (\textit{e.g.}, ``go''), or use a keyboard shortcut to indicate that they are finished speaking. Again, this adds an extra step to the process but would likely prevent errors and remove the pressure some participants felt to rush to finish speaking.

\subsubsection{How much to show users and when?}
A few participants noted that while they were speaking their query, they would watch its transcription appear in ReMap's search field. This may have been distracting, as it took their attention away from the task at hand and made deixis feel less natural. Indeed, Kalyuga \textit{et al.} \cite{Kalyuga1999} showed that splitting one's attention in the same modality increases cognitive load; in this case, user's visual attention was split between the software and ReMap's search field. We have since updated ReMap with a setting that can be toggled to specify whether to show the query as it is transcribed or only when it is finished; future work should compare these to see whether one consistently performs better, or if personal preference should dictate the setting. Seeing one's speech transcribed in real time (as many mobile voice assistants do) assures the user that the system is actively listening and gives them immediate feedback as to whether their speech has been accurately transcribed. But in the case where the search interface is not the primary application being used, it may add more distraction than the assurance is worth. 

\subsubsection{How to correct mistakes in a speech query?}
In general, the problem of making a correction to a speech command or query is a difficult one \cite{Myers2018, Jiang2013, Paek2008}. Errors may be caused by the system (\textit{e.g.}, speech transcription errors) or by the user (\textit{e.g.}, saying a word they didn't intend). When such errors occurred with ReMap, participants mostly resorted to either editing the query manually by typing or repeating the entire query over again, echoing previous findings \cite{Myers2018, Jiang2013}. One participant corrected their query in real time by repeating it while speaking, much as one might correct oneself in normal conversation, which led to the entire utterance being transcribed as one long query. A smarter system might recognize this repetition and automatically use the corrected version only. Future work should explore how this and other methods for correction might be implemented. For example, Jiang \textit{et al.} \cite{Jiang2013} recommend letting users specify and repeat only a certain portion of a query that they want to correct. Shokouhi \textit{et al.} \cite{Shokouhi2014} have shown that (at least on mobile) people prefer not to switch between speech and text when correcting a query, so one option could be to support speech commands such as ``change X to Y'' that allow users to replace incorrect words using voice alone.



\subsection{Improving the Robustness of System-wide Contextual Support}
ReMap contributes the first system-wide means for multimodal contextual help search. It achieves this by leveraging accessibility \textsc{api}s to obtain system-wide information about the user's actions. However, this approach only works when accessibility \textsc{api}s provide the necessary information, which they sometimes do not. This section discusses how future work might address these gaps.
% Our experience building ReMap led to several important implementation challenges that would need to be addressed before a system like this could be widely released.

\subsubsection{Limitations of current accessibility APIs}
ReMap is only able to detect interface elements that have been explicitly labeled with accessibility information by the application developer, which as both RePlay and prior work \cite{Hurst2010, Chang2011} have shown, varies widely across applications. This chapter's study also found that even in one of the most thoroughly labeled creative software applications (Canva), some accessibility labels were still missing (\textit{e.g.,} charts on the canvas). In general, accessibility labeling tends to be more common for interface ``widgets'' such as menu items and tools, and less common for the ``insides'' of applications such as editing areas or canvases \cite{Hurst2010}. However, there are existing efforts in the research community to make applications and systems more accessible using crowdsourcing and computer vision \cite{Guo2016, Guo2019StateLens}, so accessibility labeling may improve in the future.

ReMap's reliance on accessibility \textsc{api}s also complicates its setup and implementation substantially. As discussed in the Implementation (\autoref{sec:remap_implementation}), ReMap needed to be built as a MacOS application in order to access the MacOS Accessibility \textsc{api}, but this required building separate webpages to handle video playing and speech detection, as well as a custom server to communicate between the webpages and ReMap. In our pilot tests and study, this setup generally worked well, although occasional delays in speech transcription did happen. However, opening a page in the web browser to detect speech would be impractical if a system like ReMap were to be deployed more widely. How else might we enable both multimodal support and detection of contextual information?

\subsubsection{Leveraging Accessibility on Mobile Systems}
ReMap may work more reliably as a mobile application, because mobile operating systems such as Android and iOS tend to have more accessibility features built in and more standardized interface components in their applications. Prior work has shown how mobile applications can leverage accessibility information to enable programming by demonstration \cite{Li2017}; a mobile version of ReMap could similarly use accessibility to provide contextual support across tablet and smartphone applications (which are increasingly used for creative work, as evidenced by the increasing prevalence of mobile apps from companies like Adobe\footnote{\url{https://www.adobe.com/ca/creativecloud/catalog/mobile.html}}). In addition, speech and pointing are already common modes of interaction with mobile devices, which may make ReMap's multimodal interaction feel more natural. 

\subsubsection{Computer Vision for Detecting Interface Elements}
Prior work has used computer vision as an alternative to accessibility \textsc{api}s for recognizing interface elements \cite{Chang2011, Hurst2010, Dixon2010}. Future work should explore how computer vision might enable system-wide contextual assistance instead of relying on accessibility labels. Computer vision may in many cases be \textit{more} effective than accessibility as it could allow for higher-level interpretation of the elements being clicked. As our studies of RePlay and ReMap found, participants rarely found tool-level context useful for including in their search query. Even when accessing commands directly, Bota \textit{et al.} \cite{Bota2018} found that people often search for actions or outcomes rather than command names. ReMap's addition of canvas elements helped make deictic resolution useful, as most deictic references participants made were for canvas elements. However, even when such elements have accessibility labels, they tend to be very general. Higher-level semantics about the element may be more useful; for example, a user might reference a photo of a sky when searching for help, in which case knowing that it shows a sky might be more useful than just knowing that it is a photo. 

Of course, computer vision may also have limitations compared to accessibility \textsc{api}s; the description or meaning of something is not always apparent from its visual attributes alone. For example, many tools in creative applications are represented by icons only; their names are not displayed. Or, the action a tool performs may not always be apparent from its visible name.

\subsubsection{System-wide Support vs. Detailed Contextual Knowledge}
ReMap illustrates both the benefits and drawbacks of system-wide, application-general assistance. While it provides users with a consistent interface for finding help across multiple applications, it is not able to tailor itself to a specific application the way an integrated plugin could. We believe the benefits outweigh the costs, especially given the potential improvements mentioned above that could be made to ReMap's approach for gathering contextual information. Nonetheless, one potential middle ground could be to build ReMap as a browser extension (similar to CheatSheet \cite{Vermette2015}). As web applications are becoming more prevalent than desktop software (with some laptops running \textit{only} web applications, \textit{e.g.,} Chromebooks), this might be a fair compromise. A browser extension could use voice recognition directly via the Web Speech \textsc{api}, enable deictic resolution and context recognition by accessing a webpage's document structure, and provide consistent support across multiple applications (as long as they are on the web).


% maybe include below in paper...
% \subsection{Translating Multimodal Input Into an Effective Search Query}
% Since ReMap leverages a commercial search engine, it must ultimately produce a textual search query to send to the engine. This means translating the user's gestures, deictic references, and context into a single string, where every keyword has a roughly equal weight (depending on the search engine's particular algorithm). But what if a search engine could consider these different input sources individually? Might that improve the quality of results? For example, DeepStyle \cite{Tautkute2019} allows users to search for fashion items by providing both text input and visual examples; its custom search engine is able to compare visual and stylistic similarity of the user's input examples with its corpus. ReMap could similarly compare visual attributes of the user's interface and documents with online videos.