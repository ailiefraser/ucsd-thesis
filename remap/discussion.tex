\section{Discussion: The Potential of Multimodal Help Search}
Overall, most participants were positive about multimodal video search, and many of the challenges they exhibited stemmed from either implementation issues or unfamiliarity with using speech and pointing for search and navigation. It is likely that an improved implementation combined with more time spent using ReMap would further increase users' success. This section outlines how ReMap's implementation and usability could be improved based on the study findings.

\subsection{Usability Challenges With Speech for Search}
\subsubsection{The Midas Touch Problem: How best to initiate a search?}
Since ReMap's speech detection is always on, participants occasionally encountered a ``Midas touch'' problem \cite{Jacob1990} of searching unintentionally. This is a challenge with a system that is always listening; personal assistants that can be activated via a voice command (\textit{e.g.}, ``Hey Siri'') cause similar accidents \cite{Hern2019}. On the other hand, needing to press a button to engage speech recognition can be burdensome, and in our study, accidental searches only happened 3 times out of all 121 speech queries issued. ReMap's current design (like many personal voice assistants today) was informed largely by the goal to make searching as quick and easy as possible, but for something as intentional as a search query, the added burden of pressing a button may still be more convenient than the risk of error. Future work should compare ReMap's current design of speaking a keyword to search with a button or keyboard shortcut to activate speech detection. 

\subsubsection{Pre-emptive searching: How to indicate a query is finished?}
Several participants were frustrated by ReMap issuing a search before they had finished speaking their query, usually because they had paused briefly to think about what to say next and the Web Speech \textsc{api} interpreted this as the end of a phrase. Most voice assistants have this same feature of automatically detecting when the user is finished speaking (\textit{e.g.}, Siri, Alexa), but they do sometimes fail and cut the user off early. A study by Jiang et al. \cite{Jiang2013} found that these ``system interruptions'' accounted for about 10\% of all transcription errors. In our study, they accounted for about 24\% (7/29) of all transcription errors, including speech recognition errors and unintentional searches. Similar to the alternatives proposed above, future work should explore whether it is preferable to require the user to explicitly press a button, say a keyword (\textit{e.g.}, ``go''), or use a keyboard shortcut to indicate that they are finished speaking. Again, this adds an extra step to the process but would likely prevent errors and would remove the pressure some participants felt to rush to finish speaking.

\subsubsection{How much to show users and when?}
A few participants noted that while they were speaking their query, they would watch its transcription appear in ReMap's search field. This may have been distracting, as it took their attention away from the task at hand and made deixis feel less natural. Indeed, Kalyuga et al. \cite{Kalyuga1999} have shown that splitting one's attention in the same modality increases cognitive load; in this case, user's visual attention was split between the software and ReMap's search field. We have since updated ReMap with a setting that can be toggled to specify whether to show the query as it is transcribed or only when it is finished; future work should compare these to see whether one consistently performs better, or if personal preference should dictate the setting. Seeing one's speech transcribed in real time (as many mobile voice assistants do) assures the user that the system is actively listening and gives them immediate feedback as to whether their speech has been accurately transcribed. But in the case where the search interface is not the main tool being used, it may add more distraction than the assurance is worth. 

\subsubsection{How to correct mistakes in a speech query?}
In general, the problem of making a correction to a speech command or query is a difficult one \cite{Myers2018, Jiang2013, Paek2008}. Errors may be caused by the system (\textit{e.g.}, speech transcription errors) or by the user (\textit{e.g.}, saying a word they didn't intend). When such errors occurred with ReMap, participants mostly resorted to either editing the query manually by typing or repeating the entire query over again, echoing previous findings \cite{Myers2018, Jiang2013}. One participant corrected their query in real time by repeating it while speaking, much as one might correct oneself in normal conversation, which led to the entire utterance being transcribed as one long query. A smarter system might recognize this repetition and automatically use the corrected version only. Future work should explore how this and other methods for correction might be implemented. For example, Jiang et al. \cite{Jiang2013} recommend letting users specify and repeat only a certain portion of a query that they want to correct. Shokouhi et al. \cite{Shokouhi2014} have shown that (at least on mobile) people prefer not to switch between speech and text when correcting a query, so one option could be to support speech commands such as ``X instead of Y'' that allow users to replace incorrect words using voice alone.

\subsection{Implementation Challenges of Combining Contextual and Multimodal Support}
Our experience building ReMap led to several important implementation challenges that would need to be addressed before a system like this could be widely released. 

\subsubsection{Limitations of Accessibility APIs}
RePlay (and thus ReMap) was built as a MacOS app specifically so that we could use the Accessibility \textsc{api} to detect user activity in other applications. However, the only available speech service for MacOS is command recognition (recognizing spoken commands from a pre-defined list). To enable searching by speech, we needed a dictation or speech-to-text service that could transcribe arbitrary words. To enable this, we used the Web Speech \textsc{api}, which requires the user to have a browser window open and relies on a steady internet connection for ReMap and the speech client to communicate. In our pilot tests and study, this setup generally worked well, although some delays in speech transcription did happen. It also complicates ReMap's setup and implementation significantly, which leads to the question: how important is Accessibility \textsc{api} access?

Even with full access to the Accessibility \textsc{api}, ReMap is only able to detect what objects have been explicitly labeled with accessibility information by the application developer, which as RePlay showed (Chapter \ref{chapter:replay}) , varies widely. Integrated application plugins could tailor themselves to a specific application's interface, but would lose the benefit of supporting people across different applications. As web applications become more prominent than desktop software, a browser extension that has access to page structure might be a fair compromise, allowing voice recognition directly and supporting people across multiple applications (as long as they are on the web). 

Some prior work has used computer vision as an alternative to accessibility \textsc{api}s for recognizing interface elements \cite{Chang2011, Hurst2010, Dixon2010}. Future work should explore how computer vision could help improve deictic references in software. Computer vision may in many cases be more effective than accessibility as it could allow for higher-level interpretation of the elements being clicked. As both the studies from RePlay and ReMap found, participants rarely found tool-level context useful for including in their search query. Even for accessing commands directly, Bota et al. \cite{Bota2018} found that people often search for actions or outcomes rather than command names. ReMap's addition of canvas elements helped make deictic resolution useful, as most deictic references participants made were for canvas elements. Even still, the accessibility description of such elements tends to be very general, and higher-level semantics about the element may be more useful. For example, a user might point at a photo of a sky, in which case knowing that it shows a sky might be more useful than just knowing that it is a photo. 

\subsection{Translating Multimodal Input Into a Textual Search Query}
Since ReMap takes advantage of a commercial search engine, it must ultimately produce a textual search query for the system. This means translating the user's gestures, deictic references, and context into a single string, where every keyword has a roughly equal weight (depending on the search engine's particular algorithm). But what if a search engine could consider these different input sources individually? Might that improve the quality of results? 

For example, DeepStyle \cite{Tautkute2019} allows users to search for fashion items by providing both text input and visual examples; its custom search engine is able to compare visual and stylistic similarity of the user's input examples with its corpus. ReMap could similarly compare visual attributes of the user's interface and documents with online videos.