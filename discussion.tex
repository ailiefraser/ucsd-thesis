\chapter{Conclusion and Future Work}
\label{chapter:discussion}
\begin{quote}
This dissertation demonstrated how expert help and demonstrations can be reused, repurposed, and curated to provide personalized contextual assistance to people doing creative work. The development and evaluation of four interactive systems showed that curating existing expert help and demonstrations and presenting them to users in context helps novices build confidence, accomplish tasks, and produce higher-quality creative work. This chapter proposes future directions based on the challenges and open questions that emerged from this research.
\end{quote}

\section{Future Directions}
\subsection{Multimodal Interaction for Integrating Resources Into Creative Work}
This section discusses how future systems might integrate help resources more closely into the user's workflow by leveraging multimodal interaction. Specifically, our experience developing and testing ReMap surfaced some challenges with using speech and pointing for complex search tasks. However, with careful design, multimodal systems could enable more natural interaction with help resources so users can more easily understand processes and achieve complex outcomes.

\subsubsection{Conversational Interaction Leverages the Benefits of Speech}
Designing an effective speech-based interface requires careful attention to how speech is used \cite{Hugunin1997}. Simply speaking a search query out loud may not be the best way to maximize users' cognitive abilities, as it gives the user only one chance to describe their intent. Interacting with an expert in-person typically involves back-and-forth dialogue to clarify and update intentions; a conversational system (\textit{e.g.}, \cite{Hugunin1997, Manuvinakurike2018}) may therefore be a more effective design for systems like ReMap. For example, Manuvinakurike \textit{et al.} \cite{Manuvinakurike2018} propose a conversational approach for editing photos where users can incrementally update results through dialogue. This allows users to achieve complex effects without having to figure out complex interfaces or learn technical terminology.

Several ReMap participants found that typing their query was easier than speaking it as they could refine and reword it while typing. Speech as a parallel input modality while working on software tasks may be better suited for short commands (\textit{e.g.}, ``rotate the image'') rather than long, descriptive queries (\textit{e.g.}, ``how do I add a title on top of a photo and position it in the center and make the text stand out''). Fittingly, people who use voice assistants on desktop issue short commands more often than search queries \cite{Mehrotra2016}. Speaking is convenient when one's hands are busy with a motor task that requires little focus (\textit{e.g.}, washing the dishes), but speaking at the same time as doing other text-based activities is often difficult because they both use verbal working memory \cite{Shneiderman2000a, Begel2006}. A conversational approach would allow users to interact more naturally with the system, giving details when they think of them. As smart assistants grow in popularity and ability, systems that converse with users while also leveraging relevant context will be increasingly useful and valuable, as they provide a familiar form of interaction with gentle learning curves \cite{Klopfenstein2017} that brings us one step closer to the ideal experience of one-on-one tutoring \cite{Bloom1984}.

\subsubsection{How to Better Leverage the Specificity of Deictic Resolution}
Arguably the most exciting piece of ReMap was its ability to process deictic references by the user. This enabled a form of ``chunking,'' \cite{Buxton1986} allowing users to point at relevant elements instead of having to describe them in words. However, in our study (Section \ref{sec:remap_study}), people rarely made deictic references, and the ones they did make did not seem to have a significant impact on the usefulness of search results. Why?

I believe the lack of utility for deictic references is largely due to most queries being goal-oriented (rather than tool- or object-oriented) and a concern of being \textit{too} specific in a search query. First, the evaluation of RePlay (Section \ref{sec:replay_study}) as well as previous research \cite{Bota2018} found that people mention \textit{actions} or \textit{goals} more than tools in search queries. While deictic resolution might be more useful in software with more complex terminology than Canva, it still does not help users describe what they \textit{want} to do. When users have a high-level goal, they often do not know what tools they even need, so referencing tools in a query is not helpful. In the ReMap study, participants who made deictic references mostly used them to refer to objects on the canvas, but in most cases the names of such objects were straightforward (\textit{e.g.}, photo, text) and so the feature did not save users much time.

Second, adding more specificity to a search query is not always helpful, as it relies on there being existing resources that match the specifics of the search query. For example, when pilot testing ReMap with iMovie, we would often make deictic references to various pieces of the project timeline, which added terms like ``video filmstrip'' and ``audio waveform'' to the query. However, help resources do not typically use these terms (opting for simpler words such as ``timeline''), and so the results from these searches were fewer and less useful. People often resort to tutorials that show how to do a similar task to their own because an exact match does not exist \cite{Lafreniere2014a}. So, describing a task in more exact terms (\textit{e.g.}, ``how to align the center of heading text with a rectangle below it'') may bring few results, when a more general description (\textit{e.g.}, ``how to align objects'') would be better.

Adding specificity via deictic resolution may be more beneficial for systems where the user is either issuing commands directly to the system (\textit{e.g.}, PixelTone \cite{Laput2013}), or asking questions to other humans (\textit{e.g.}, CodeOn \cite{Chen2017}). However, relying on other humans will inevitably result in a delayed response when someone is not immediately available, and the solution the user desires may already exist on the web somewhere else. And a system that responds automatically to commands will inevitably encounter situations where it does not understand the user's command. One way to mitigate this is to have the user teach the system: for example, the mobile system \textsc{sugilite} \cite{Li2017} allows users to issue voice commands like other mobile voice assistants, but when it does not know how to do what the user asks, the user can demonstrate and the system learns for next time. But what about when the user does not know how to do what they are asking?

\subsubsection{A Multimodal System That Adapts to the User's Goal}
Ideally, a system for contextually presenting expert resources could leverage the strengths of both speech and deictic interaction by adapting based on the goals of the user and the available information. As the line between search engines and smart assistants continues to blur, people use such systems for both executing commands and issuing natural language search queries \cite{Bota2018, Adar2014, Fourney2016, Norman2007}. A system that supports both would allow users to better leverage the benefits of deictic resolution for specifying their requests. 

If the user makes a request the system has not seen before, it could search the web for related tools or instructions and automatically adapt results to the user's particular request and context. If no relevant web results exist, the system could send the user's question to available experts (from discussion forums or other community connections) for help \cite{Ackerman1990, Chen2017, Kim2012}. Over time, the system could learn from the user's actions in response to web search results and expert responses to build up a library of user requests and corresponding actions, thus leveraging the knowledge of online experts and communities in a more robust and extensible way. Then, if a user makes a request the system has seen before, it could automatically execute the command (if the user wants a quick outcome) or walk the user through instructions (if the user wants to understand the process). In the latter case, the system could help the user follow along with instructions by highlighting mentioned tools in the software's interface or even overlaying images and videos on top of relevant parts of the interface. The system could engage the user in conversational dialogue to confirm parameter settings or clarify the request. 
%For creative tools especially, allowing users to smoothly move between issuing commands and walking through procedural instructions will be particularly important, as users desire control over their work for it to truly feel creative \cite{?}.

To inform the design of such a system, future work should first explore in more detail how people ideally expect a multimodal help system to function and behave. This could be accomplished by conducting Wizard-of-Oz studies where participants use a multimodal ``system'' that responds to any request they make. A researcher could serve as the ``wizard'' and control the system behind the scenes to accomplish whatever participants ask for. Another reason ReMap participants may not have used deictic resolution as much as they otherwise would have could be that they lost trust in the feature after trying it once or twice and failing to make a successful deictic resolution (due to the system's limitations). A Wizard-of-Oz study would mitigate this concern by not relying on a system to understand user's deictic references or speech commands, instead exploring how users would behave with an ``ideal'' system. One could also vary the tasks required of participants from open-ended tasks (like creating abstract art) to specific tasks that require participants to produce a given outcome (like the design re-creation task used to evaluate ReMap). This would help us understand how users' needs vary depending on their task, to inform the design of an optimally helpful creativity support system.

\subsection{Enabling Better Understanding of User Context and Expert Resources}
The systems presented in this dissertation all rely on having some semantic knowledge about the expert resources used (in order to curate and present them in a useful way) as well as contextual knowledge about the user's environment and task (in order to find relevant expert resources). Each system had limitations in how such knowledge was gathered and used automatically; most available data was low-level (\textit{e.g.}, usage logs) but higher-level knowledge about the user's task and intent might be more useful for connecting users with relevant resources. Indeed, understanding task-level semantic information about both user behaviour and online resources such as videos is still a largely unsolved problem in the research community. This section discusses the challenges and possibilities for obtaining such knowledge, as well as the trade-offs that emerge between application-general and application-specific solutions.

\subsubsection{Obtaining Task-Level User Context}
Like with deictic resolution, too much context can sometimes add too much specificity or noise to a search query and degrade the quality of results \cite{Finkelstein2002, Ekstrand2011}. RePlay and ReMap used context not just for augmenting search queries, but also for determining relevant moments within videos, inspired by Ekstrand \textit{et al.} \cite{Ekstrand2011}. But the only type of context RePlay and ReMap used were the user's recent tools. As both studies found, tool-level context is often not helpful for novices. There are many more types of context that may be useful for search, such as application state, details about the user's document, and context beyond the application such as system-level settings or the user's prior knowledge \cite{Ekstrand2011}. Higher-level semantic context such as the type of document open or the type of task being attempted might be more useful. Future work should explore how to identify such context and add it to search queries when appropriate. 

Automatically obtaining a task-level understanding of the user's context and intent is a longstanding and difficult problem, but some recent work has begun to explore solutions. Liu \textit{et al.} \cite{Liu2020} proposed a machine learning approach for identifying a hierarchy of subtasks from low-level usage logs of image editing tasks. This data-driven approach could likely be extended to other types of creative software tasks. One could also compare low-level usage logs to a large corpus of usage logs with accompanying videos, like Wang \textit{et al.} \cite{Wang2018}. If such videos also include descriptions or audio narration, this information could be used to generate a mapping between low-level usage and natural language activity descriptions, which could in turn be used to improve users' search queries. Another promising method for inferring higher-level context from usage might be computer vision, which has been successfully used to identify interface elements \cite{Chang2011, Hurst2010, Dixon2010}. Comparing the visual properties of the user's document and interface with those in existing online resources could help identify the most similar resources to the user's activity.

\subsubsection{Understanding the Contents of Expert Resources}
The expert resources we gathered for Discovery\-Space and CritiqueKit were manually labeled with semantic information (tags describing actions in Discovery\-Space and presence of feedback characteristics in CritiqueKit). RePlay, ReMap, and LiveClips leveraged pre-existing metadata (video captions for RePlay and ReMap and time-stamped usage logs for LiveClips), but even with these data sources, there were several important limitations in what we could infer from the videos. Addressing these limitations might help such systems provide more useful results to users and improve the ways people navigate within results.

A challenge we encountered in the development of RePlay, ReMap, and LiveClips was how to understand the contents of screencast videos. Most online screencast videos do not include accompanying usage logs, and although some platforms (like YouTube) automatically generate captions, we found that they often include mistakes, especially for domain-specific terminology that is uncommon in everyday speech. Many other video platforms do not generate captions at all. Prior work has demonstrated how computer vision can extract usage information from screencast videos \cite{Banovic2012, Pongnumkul2011} but these methods miss invisible operations, such as keyboard shortcuts and changes of state that do not cause visible change. Similarly, extracting tool information from text-based tutorials is possible \cite{Pavel2013, Fourney2014Mining, Fourney2012, Fourney2011} but misses any operation not explicitly described. 

Usage logs and tool mentions are helpful for identifying specific moments of tool use, but what users more often need when browsing tutorials is higher-level information about the steps in the task \cite{Pongnumkul2011, Harrison1995, Chi2012, Kim2014}. While text-based tutorials are often structured based on steps (\textit{e.g.}, \href{https://www.instructables.com/}{\nolinkurl{instructables.com}}), in videos this structure is often implicit and therefore not surfaced in most video-viewing interfaces \cite{Pavel2014}. Determining task-level structure from videos with or without usage logs is still a challenge \cite{Pongnumkul2011, Chi2012, Grossman2010}, even with the help of audio transcripts \cite{Fraser2020}. Some systems leverage the crowd to generate higher-level labels for videos \cite{Kim2014, Weir2015}, but doing this for every new video that gets uploaded takes time, resources, and moderation. 

What if instead of reverse engineering expert resources, we could give the expert creators simple low-cost ways to annotate and label their own resources \textit{while} they make them? No automated method will ever match ground truth perfectly, but if adding structure to help and demonstrations was as easy as making them in the first place, perhaps more creators would do so. For example, video creators could say certain keywords out loud every time they start a new subtask, and video browsing interfaces like RePlay could use these keywords to display higher-level segmentations of videos to viewers. Or, people recording action macros could describe what they are doing out loud while they record, and action recommendation systems like Discovery\-Space could analyze those transcripts to determine descriptive keywords for each action, and even provide users with explanations of particular steps if they are interested. A major challenge with implementing such a strategy would be coming up with a standard that multiple resource-sharing platforms would leverage, so that enough segmented expert resources are available. A major advantage of leveraging YouTube videos for RePlay and ReMap or action macros on the web for Discovery\-Space was that these are existing corpora with a large variety of expert resources.

\subsubsection{Application-General vs. Application-Specific Solutions}
The systems presented in this dissertation illustrate the trade-offs between system-wide solutions that provide contextual support across multiple applications (RePlay and ReMap) and application-specific solutions that are integrated into a single application for a single domain (LiveClips, DiscoverySpace, and CritiqueKit). 

Using accessibility \textsc{api}s to build an application-general solution for RePlay and ReMap meant that important contextual information was often missing, for example when users tried to make deictic references to objects that did not have accessibility labels. It also required a more complicated implementation. However, RePlay and ReMap enabled consistent support for cross-application workflows, and their general solution of indexing into videos using captions was able to find relevant moments reasonably well.

On the other hand, integrating LiveClips and Discovery\-Space directly into Photoshop meant that they had more information about the user's behaviour and context. We were able to tailor LiveClips' approach for segmenting videos more specifically to the domains of photo editing and illustration, making assumptions about the types of actions that were present in videos (\textit{e.g.,} zooming and panning). However, both LiveClips' algorithm for segmenting videos and Discovery\-Space's algorithm for recommending actions would need to be modified to apply to other types of creative tasks, and the user loses access to both systems' contextual support as soon as they leave Photoshop.

As efforts to improve accessibility labeling continue to grow (\textit{e.g.,} \cite{Guo2016, Guo2019StateLens}) and methods for automatically understanding user interfaces and behaviour via computer vision and machine learning become more sophisticated (\textit{e.g.,} \cite{Liu2020, Dixon2010}), system-wide support will likely become more reliable and robust in the future. However, there will always be some benefit to tailored, domain-specific solutions that leverage an application's particular features. This dissertation demonstrated how in either case, targeting help to the user's context and leveraging expert resources from online communities are key strategies for supporting creative software users in their work.

\subsection{Improving Navigation and Browsing of Video Resources}
Presenting videos in context has limitations, especially space-wise. There is only so much detail one can show when displaying videos alongside the user's work, as they risk obstructing important information if they take up too much space. This was a challenge when designing RePlay, ReMap, and LiveClips. We had to either shrink the entire video to fit inside the contextual interface, which risks making important details too small to see; or crop videos to a smaller area, which risks removing potentially useful content. In both cases, our systems allowed users to optionally open the video in a larger window, but we did not explore how to take advantage of this extra space to ease navigation and browsing. We have so many useful ways of structuring, organizing, and searching through text resources, but videos still remain difficult to skim and navigate. How might we make videos easier to watch when we are \textit{not} restricted by space, for example in a web browser? 

As discussed in Chapter \ref{chapter:related_work}, prior work has developed approaches for navigating videos using timeline markers \cite{Matejka2011, Grossman2010, Kim2014, Banovic2012, Kim2014a}, thumbnail images \cite{Kim2014a, Pongnumkul2011, Banovic2012, Grossman2010a, Chi2012, Pavel2014}, transcript text \cite{Kim2014a}, and clickable elements overlaid on the video \cite{Nguyen2015}. For other types of videos beyond software screencasts, even more creative solutions exist, such as navigating videos by direct manipulation of objects in the video \cite{Goldman2008}, and summarizing videos as concept maps \cite{Liu2018} and tapestries \cite{Barnes2010}. How might we adapt these ideas to creative software videos while leveraging their associated metadata? Are there more useful ways to summarize and display videos than as a single image with a linear timeline? As videos become an increasingly popular medium for consumption in all kinds of domains, future work should continue to explore these questions.

\subsection{Applicability to Domains Beyond Creative Software Activities}
This dissertation focuses specifically on creative software tasks, but the proposed approaches may also apply to any complex task for which expert examples or resources exist. For example, productivity applications like Microsoft Excel are not typically described as creative but exhibit many of the challenges of complex software, and also have an abundance of resources online, including tutorials and macros\footnote{For example, \href{https://excelchamps.com/blog/useful-macro-codes-for-vba-newcomers/}{\nolinkurl{excelchamps.com/blog/useful-macro-codes-for-vba-newcomers}}}. Systems such as Wrangler \cite{Kandel2011} have shown how contextual suggestions can benefit users of data processing software; enhancing such assistance with expert help and demonstrations may therefore also be helpful.

Beyond software, online help and demonstrations also abound for physical tasks, such as crafting and DIY construction (\textit{e.g.}, \href{https://www.instructables.com/}{\nolinkurl{instructables.com}}, \href{https://www.pinterest.com/}{\nolinkurl{pinterest.com}}). The methods presented in this dissertation for searching and curating online resources may also hold for such tasks, but the remaining challenge would be to understand the user's context and activity when it takes place outside of a computer. Prior research has introduced methods for detecting physical tool use by augmenting tools with sensors \cite{Schoop2016, Antifakos2002}, detecting physical user activity with sensors worn on the body \cite{Lukowicz2004}, tracking physical objects using head-worn cameras \cite{Henderson2011}, and detecting user movement using depth sensors \cite{Anderson2013}. Future work should explore how we might leverage such information for finding relevant expert resources and presenting them to users in context.

% \section{Limitations?}
% Should I include this?

% With the exception of CritiqueKit, the studies in this dissertation did not directly measure the quality of work produced. We expect that by improving parts of the process (\textit{e.g.}, increasing confidence with Discovery\-Space or reducing time away from task with RePlay), the overall improvement will follow, but future studies should confirm whether this is true. Measuring creative output is difficult...
% include creativity support tools 07 challenges of lab studies from liveclips limitations

\section{Closing Remarks}
This dissertation introduced four different approaches for curating existing expert help and demonstrations and presenting them to users in context. It contributed algorithms for curating and ranking four different types of expert resource (tutorial videos, live stream videos, action macros, and written feedback), interfaces for presenting curated resources in context, evaluations demonstrating the efficacy of these algorithms and interfaces, and formative knowledge about peoples' use of creative software and resources. These contributions demonstrate the potential of bringing knowledge from user communities to everyone's fingertips in a personalized, contextual way. As creative software continues to be more available and plentiful than ever, my hope is that by leveraging the wealth of knowledge that already exists online, future systems can make creative work less solitary and more collaborative, allowing people to learn from and inspire each other.
