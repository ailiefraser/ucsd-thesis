\chapter{Conclusion and Future Work}
\label{chapter:discussion}
This dissertation demonstrated how expert help and demonstrations can be reused, repurposed, and curated to provide personalized contextual assistance to people doing creative work. The development and evaluation of four interactive systems showed that curating existing expert help and demonstrations and presenting them to users in context helps novices build confidence, accomplish tasks, and produce higher-quality creative work. This chapter proposes future directions based on the challenges and open questions that emerged from this research.

\section{Future Directions}
\subsection{Using Multimodal Input for Both Process and Outcomes}
Based on our experience developing and testing ReMap, this section discusses how input modalities like speech and pointing may be more naturally suited for issuing short commands, but with careful design, systems may be able to leverage multimodal input to help users more easily understand processes and achieve complex outcomes.

\subsubsection{Leveraging the Benefits of Speech}
Designing an effective speech-based interface requires careful attention to how speech is used \cite{Hugunin1997}. Simply speaking a search query out loud may not be the best way to maximize users' cognitive abilities, as it gives the user only one chance to describe their intent. Interacting with an expert in-person typically involves back-and-forth dialogue to clarify and update intentions; a conversational system (\textit{e.g.}, \cite{Hugunin1997, Manuvinakurike2018}) may therefore be a more effective design for systems like ReMap. For example, Manuvinakurike \textit{et al.} \cite{Manuvinakurike2018} propose a conversational approach for editing photos where users can incrementally update results through dialogue. This allows users to achieve complex effects without having to figure out complex interfaces or learn technical terminology.

As several ReMap participants found, typing their query was easier than speaking it as they could refine and reword it while typing. Speech as a parallel input modality while working on software tasks may be better suited for short commands (\textit{e.g.}, ``rotate the image'') rather than long, descriptive queries (\textit{e.g.}, ``how do I add a title on top of a photo and position it in the center and make the text stand out''). Fittingly, people who use voice assistants on desktop issue short commands more often than search queries \cite{Mehrotra2016}. Speaking is convenient when one's hands are busy with a motor task that requires little focus (\textit{e.g.}, washing the dishes), but speaking at the same time as doing other text-based activities is often difficult because they both use verbal working memory \cite{Shneiderman2000a, Begel2006}. A conversational approach would allow users to interact more naturally with the system, giving details when they think of them. As smart assistants grow in popularity and ability, systems that converse with users while also leveraging relevant context will be increasingly useful and valuable, as they provide a familiar form of interaction with gentle learning curves \cite{Klopfenstein2017}.

\subsubsection{Leveraging the Specificity of Deictic Resolution}
Arguably the most exciting piece of ReMap was its ability to process deictic references by the user. This enabled a form of ``chunking,'' \cite{Buxton1986} allowing users to point at relevant elements instead of having to describe them in words. However, in our study (Section \ref{sec:remap_study}), people rarely made deictic references, and the ones they did make did not seem to have a significant impact on the usefulness of search results. Why?

I believe the lack of utility for deictic references is largely due to most queries being goal-oriented (rather than tool- or object-oriented) and the danger of being \textit{too} specific in a search query. First, the evaluation of RePlay (Section \ref{sec:replay_study}) as well as previous research \cite{Bota2018} found that people mention \textit{actions} or \textit{goals} more than tools or commands in search queries. While deictic resolution might be more useful in software with more difficult terminology than Canva, it still does not help users describe what they \textit{want} to do. When users have a high-level goal, they likely do not know what tools they even need, so referencing tools in a query is not helpful. In the ReMap study, participants who made deictic references mostly used them to refer to objects on the canvas, but in most cases the names of such objects were straightforward (\textit{e.g.}, photo, text) and so the feature did not save users much time.

Second, adding more specificity to a search query is not always helpful, as it relies on there being existing resources that match the specifics of the search query. For example, when pilot testing ReMap with iMovie, we would often make deictic references to various pieces of the project timeline, which added terms like ``video filmstrip'' and ``audio waveform'' to the query. However, help resources do not typically use these terms (opting for simpler words such as ``timeline''), and so the results from these searches were fewer and less useful. People often resort to tutorials that show how to do a similar task to their own because an exact match does not exist \cite{Lafreniere2014a}. So, describing a task in more exact terms (\textit{e.g.}, ``how to do something really specific'') may bring no results, when a more general description (\textit{e.g.}, ``how to do related thing'') would be better.

Adding specificity to queries via deictic resolution may be more beneficial for systems where the user is either issuing commands directly to the system (\textit{e.g.}, PixelTone \cite{Laput2013}), or asking questions to other humans (\textit{e.g.}, CodeOn \cite{Chen2017}). However, relying on other humans will inevitably result in a delayed response when someone is not immediately available, and the solution the user desires may already exist on the web somewhere else. And a system that responds automatically to commands will inevitably encounter situations where it does not understand the user's command. One way to mitigate this is to have the user teach the system: for example, the mobile system \textsc{sugilite} \cite{Li2017} allows users to issue voice commands like other mobile voice assistants, but when it does not know how to do what the user asks, the user can demonstrate and the system learns for next time. But what about when the user does not know how to do what they are asking?

\subsubsection{Future Work: A Multimodal System That Adapts to the User's Goal}
Ideally, a system for contextually presenting expert resources could leverage the strengths of both speech and deictic interaction by adapting based on the goals of the user and the available information. As the line between search engines and smart assistants continues to blur, people use such tools for both executing commands and issuing natural language search queries \cite{Bota2018, Adar2014, Fourney2016, Norman2007}. A system that supports both would allow users to better leverage the benefits of deictic resolution for specifying their requests. 

If the user makes a request the system has not seen before, it could search the web for related commands or instructions and automatically adapt results to the user's particular request and context. If no relevant web results exist, the system could send the user's question to available experts (from discussion forums or other community connections) for help \cite{Ackerman1990, Chen2017, Kim2012}. Over time, the system could learn from the user's actions in response to web search results and expert responses to build up a library of user requests and corresponding actions, thus leveraging the knowledge of online experts and communities in a more robust and extensible way. Then, if a user makes a request the system has seen before, it could automatically execute the command (if the user wants a quick outcome) or walk the user through instructions (if the user wants to understand the process). The system could engage the user in conversational dialogue to confirm parameter settings or clarify the request. 

\subsection{Enabling Better Understanding of User Context and Expert Resources}
The systems presented in this dissertation all rely on having some semantic knowledge about the expert resources used (in order to curate and present them in a useful way) as well as contextual knowledge about the user's environment and task (in order to find relevant expert resources). Each system had limitations in how such knowledge was gathered and used automatically; most available data was low-level (\textit{e.g.}, command logs) but higher-level knowledge about the user's task and intent might be more useful for connecting users with relevant resources. This section discusses the challenges and possibilities for obtaining such knowledge.

\subsubsection{User context}
Like with deictic resolution, too much context can sometimes add too much specificity or noise to a search query and degrade the quality of results \cite{Finkelstein2002, Ekstrand2011}. RePlay and ReMap use context not just for augmenting search queries, but also for determining relevant moments within videos, inspired by Ekstrand \textit{et al.} \cite{Ekstrand2011}. But the only type of context RePlay and ReMap used were the user's recent tools and commands. As both studies found, tool-level context is often not needed by novices. There are many more types of context that may be useful for search, such as application state, details about the user's document, and context beyond the application such as system-level settings or the user's prior knowledge \cite{Ekstrand2011}. Higher-level semantic context such as the type of document open or the type of task being attempted might be more useful. Future work should explore how to identify such context and add it to search queries when appropriate. One promising method might be computer vision, which has been successfully used to identify interface elements \cite{Chang2011, Hurst2010, Dixon2010}. Comparing the user's visual features and movements with those in existing online resources could help identify resources with higher similarity to the user's activity. Another approach could be to compare the user's low-level usage logs to a large corpus of usage logs with accompanying videos, like Wang \textit{et al.} \cite{Wang2018}. If such videos also include descriptions or audio narration, this could be used to generate a mapping between low-level usage and natural language activity descriptions, which could in turn be used to improve textual search queries.

\subsubsection{Expert resources}
The expert resources we gathered for Discovery\-Space and CritiqueKit were manually labeled with semantic information (tags describing actions in Discovery\-Space and presence of feedback characteristics in CritiqueKit). RePlay, ReMap, and LiveClips leveraged pre-existing metadata (video captions for RePlay and ReMap and time-stamped usage logs for LiveClips), but even with these data sources, there were several important limitations in what we could infer from the videos. Addressing these limitations might help such systems provide more useful results to users and improve the ways people navigate within results.

One challenge that often arises when using web tutorials is a mismatch in software versions \cite{Lafreniere2013a}. A command shown in a tutorial may not be in the same place in the user's version, or if it is a newer command, it may not even be available for the user. Conversely, old tutorials often recommend strategies that are suboptimal in newer software (as the evaluation of Discovery\-Space found). Search systems could include the user's software version in a query to isolate results for that same version, but this only works if the resources themselves specify which version they use, which most do not (as we have found throughout the development of our systems). Future work should investigate how systems might infer this information based on available metadata.

More generally, a challenge we encountered in the development of RePlay and ReMap was how to understand the contents of screencast videos. Most online screencast videos do not include accompanying usage logs, and although some platforms (like YouTube) automatically generate captions, we found that they often include mistakes, especially for domain-specific terminology that is uncommon in everyday speech. Many other video platforms do not generate captions at all. Prior work has demonstrated how computer vision can extract usage information from screencast videos \cite{Banovic2012, Pongnumkul2011} but these methods miss commands and actions that are not visible, such as keyboard shortcuts and changes of state that may not cause visible change. Similarly, extracting command information from text-based tutorials is possible \cite{Pavel2013, Fourney2014Mining, Fourney2012, Fourney2011} but misses any action not explicitly described. 

Usage logs and tool mentions are helpful for identifying specific moments of tool use, but what users more often need when browsing tutorials is higher-level information about the steps in a task being shown \cite{Pongnumkul2011, Harrison1995, Chi2012, Kim2014}. While text-based tutorials are often structured based on steps (\textit{e.g.}, \href{https://www.instructables.com/}{\nolinkurl{instructables.com}}), in videos this structure is often implicit and therefore not surfaced in most video-viewing interfaces \cite{Pavel2014}. Determining task-level structure from videos with or without usage logs is a known challenge \cite{Pongnumkul2011, Chi2012, Grossman2010}, even with the help of audio transcripts \cite{Fraser2020}. Some systems leverage the crowd to generate higher-level labels for videos \cite{Kim2014, Weir2015}, but doing this for every new video that gets uploaded takes time, resources, and moderation. 

What if instead of reverse engineering expert resources, we could give the expert creators simple low-cost ways to annotate and label their own resources while they make them? No automated method will ever match ground truth perfectly, but if adding structure to help and demonstrations was as easy as making them in the first place, perhaps more creators would do so. For example, video creators could say certain keywords out loud every time they start a new subtask, and video browsing interfaces like RePlay could use these keywords to display higher-level segmentations of videos to viewers. Or, people recording action macros could describe what they are doing out loud while they record, and action recommendation systems like Discovery\-Space could analyze those transcripts to determine descriptive keywords for each action, and even provide users with explanations of particular steps if they are interested. A major challenge with implementing such a strategy would be coming up with a standard that multiple resource-sharing platforms would leverage, so that enough segmented expert resources are available. A major advantage of leveraging YouTube videos for RePlay and ReMap or action macros on the web for Discovery\-Space was that these are existing corpora with a large variety of expert resources.

\subsection{Applicability to Domains Beyond Creative Software Activities}
This dissertation focuses specifically on creative software tasks, but the proposed approaches may also apply to any complex task for which expert examples or resources exist. For example, productivity tools like Microsoft Excel are not typically described as creative but exhibit many of the challenges of complex software, and also have an abundance of resources online, including tutorials and macros\footnote{For example, \href{https://excelchamps.com/blog/useful-macro-codes-for-vba-newcomers/}{\nolinkurl{excelchamps.com/blog/useful-macro-codes-for-vba-newcomers}}}. Tools such as Wrangler \cite{Kandel2011} have shown how contextual suggestions can benefit users of data processing software; enhancing such assistance with expert help and demonstrations may therefore also be helpful.

Beyond software, online help and demonstrations also abound for physical tasks, such as crafting and DIY construction (\textit{e.g.}, \href{https://www.instructables.com/}{\nolinkurl{instructables.com}}, \href{https://www.pinterest.com/}{\nolinkurl{pinterest.com}}). The methods presented in this dissertation for searching and curating online resources may also hold for such tasks, but the remaining challenge would be to understand the user's context and activity when it takes place outside of a computer. Prior research has introduced methods for detecting physical tool use by augmenting tools with sensors \cite{Schoop2016, Antifakos2002}, detecting physical user activity with sensors worn on the body \cite{Lukowicz2004}, tracking physical objects using head-worn cameras \cite{Henderson2011}, and detecting user movement using depth sensors \cite{Anderson2013}. Future work should explore how we might leverage such information for finding relevant expert resources and presenting them to users in context.

% \section{Limitations?}
% Should I include this?

% With the exception of CritiqueKit, the studies in this dissertation did not directly measure the quality of work produced. We expect that by improving parts of the process (\textit{e.g.}, increasing confidence with Discovery\-Space or reducing time away from task with RePlay), the overall improvement will follow, but future studies should confirm whether this is true. Measuring creative output is difficult...
% include creativity support tools 07 challenges of lab studies from liveclips limitations

\section{Closing Remarks}
This dissertation demonstrates the potential of bringing knowledge from user communities to everyone's fingertips in a personalized, contextual way. As creative tools continue to be more available and plentiful than ever, my hope is that by leveraging the wealth of knowledge that already exists online, future systems can make creative work less solitary and more collaborative, allowing people to learn from and inspire each other.