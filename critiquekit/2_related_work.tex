\section{Related Work}
\subsection{Good Feedback is Actionable, yet Rare}
Rapid iteration is critical to the success of creative projects, from essays, to visual design, to buildings [5,35]. Receiving feedback early on is important for learners to test alternatives and course-correct [5,41]. Effective feedback is especially important in educational settings where novices are learning new skills and developing expertise. However, giving effective feedback is rarely taught [30]. As physical and digital classrooms increase in size, the demand for feedback outgrows the ability to adopt the ideal learning model of one-to-one feedback [2]. Instead, a one-to-many approach is utilized, where an expert provides feedback for multiple learners. Although learners most value expert feedback [9,27], the one-to-many approach is highly demanding on experts, and specific, actionable feedback for individuals becomes increasingly rare. 

In general, effective feedback is specific, actionable, and justified. Specific feedback is direct and related to a particular part of the work rather than vaguely referent [19,35,46]. Specific positive feedback also highlights strengths of the work and provides encouragement, so the recipient can tell they are on a good path [18,43,46]. Actionable feedback is important because it offers the learner a concrete step forward [35,40,43,46]. Simply pointing out a problem is not sufficient to help one improve [32,35,40,41]. Actionable feedback is often most helpful early in a project [4,43,46] because it may help people self-reflect and self-evaluate their work [8], prompting more revisions for improvement [6,42]. Lastly, justification is an important characteristic of feedback [19,28,46], but is arguably one of the hardest to understand or recognize [9]. Justified feedback contains an explanation or reason for a suggested change, which helps the learner understand why the feedback was given.

\subsection{Rubrics \& Examples Usefully Focus Feedback}
Rubrics [1,46] and comparative examples [19] are effective in structuring feedback because they beneficially encourage attention to deep and diverse criteria. Novices otherwise tend to focus on the first thing they notice, often surface-level details [12,17,20,46]. Viewing examples of past designs can lead to greater creativity and insights [21,26]; thus, showing examples of good feedback may spark ideas reviewers would not have otherwise considered [12,22,25]. Also, adaptive examples curated to match design features are more helpful than random examples in improving creative work [23]. 

Rubrics and other scaffolds require significant upfront manual work by experts who must carefully design a comprehensive rubric, curate a thorough set of examples, or decide how else to structure the feedback process. This paper investigates leveraging existing feedback to dynamically create rubric criteria. We hypothesize that showing reviewers previously-provided feedback can guide their attention to important aspects of the design. 
\subsection{Is Feedback too Context-specific for Practical Reuse?}
Schön persuasively argues that effective feedback should be context-specific and expert-generated [36]. He offers a vignette from architecture where the teacher suggests an alternative building to the student as an example of situated wisdom and its transfer. If Schön is right that this exchange requires both wisdom and context, does that mean that feedback reuse is infeasible? Within a given setting, project, or genre, common issues recur. Hewing to the principle of recognition over recall, we hypothesize that suggestions and guidance can increase novices' participation in context-specific exchanges. 

\subsection{Prior Systems \& Approaches for Scaling Feedback}
Existing approaches for scaling personalized feedback include clustering by similarity (\textit{e.g.}, for writing [3] and programming [10,15]). Gradescope [39] and Turnitin [47] allow graders to create reusable rubric items and comments to address common issues and apply them across multiple assignments. Gradescope binds rubric items to scores, which emphasizes grades rather than improvement. 

Other methods include automating the reuse of the solutions of previous learners. These methods work best when correct and incorrect solutions are clearly distinct, such as in programming [11,13] and logical deductions [7]. Automated methods have also found success with the formal aspects of more open-ended domains such as writing [3,34].  However, assessing the quality and effectiveness of creative work – the strength of a design, the power of a poem – is intrinsically abstract and subjective and lies beyond current automated analysis techniques. Also, little automated analysis exists for media other than text. For domains like design, human-in-the-loop analysis will remain important for quite some time. 

\subsection{Automatically Detecting Feedback Characteristics}
Although feedback is often specific and contextual [36], general characteristics can be automatically detected and used to help reviewers improve their feedback. For example, PeerStudio detects when comments can be improved based on the length of the comment and the number of relevant words [20]. Data mining and natural-language processing techniques can also automatically detect whether a comment is actionable or not, and prompt the reviewer to include a solution [29,45]. Krause et al. use a natural-language processing model to detect linguistic characteristics of feedback and suggest examples to reviewers to help them improve their comment [19]. These methods require a reviewer to first submit their comment so it can be analyzed, and then improve their comment after submission.
