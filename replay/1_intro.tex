\section{Introduction}
Most software activities span multiple applications. The slogan ``there's an app for that'' illustrates that we live in a world filled with specialized apps that each focus on a few specific tasks each. To accomplish larger activities requires composing multiple applications together into a ``toolbelt'' \cite{Sumner1997}. For example, designing an interface might comprise drawing a logo in Illustrator, mocking up a prototype in Sketch, adding animations using Flinto, and presenting it to a client using Keynote. Analyzing data might involve formatting it using Python, viewing and graphing it in Excel, modifying graph aesthetics in Photoshop, and reporting results in Word. Toolbelts help users tailor custom ecosystems and support distributed innovation. However, this bricolage creates a user experience problem: even with design guidelines, every app is different \cite{Beaudouin-Lafon2018}. As new applications appear and existing ones change, few people are fluent experts in all the steps towards their goals.

Presenting learning resources in-application \cite{Grossman2010a, Chilana2012, Matejka2011a, Matejka2011, Brandt2010, Ichinco2017} and augmenting search queries with contextual information \cite{Ekstrand2011, Brandt2010} can offer a more fluid experience with lower cognitive load. However, existing solutions require deep integration with applications. And since today's applications are ``walled gardens''  with limited integration across software vendors \cite{Beaudouin-Lafon2018}, help resources typically focus on one application at a time. This leaves gaps when users want to move from one application to another (\textit{e.g.,} export an Adobe \textsc{xd} prototype to Zeplin) or interleave applications (\textit{e.g.,} coding a website in Sublime while debugging in Chrome and resizing graphics in \textsc{gimp}). 

Web search results can of course include community-created resources that span applications. However, generic web search poses two problems. First, search is blind to relevant contextual information that could connect users to better resources \cite{Ekstrand2011, Kraft2005, Finkelstein2002}. Search engines place the burden on users to articulate an appropriate query, an almost paradoxical requirement for users who are there because they don't know the domain \cite{Russell2011}.  Second, search is divorced from the application UX, requiring users to bounce back and forth to connect the content \cite{Fourney2014Intertwine}. These challenges are amplified when users work with multiple applications, each with its own terminology and conventions.

We introduce an application-independent approach for contextually presenting video learning resources. We embody this approach in the RePlay system, which enables users to search for learning videos based on their application usage. RePlay gathers application context using system accessibility \textsc{api}s. It extends online video search and cues videos to relevant segments based on their captions. We focus on video assistance because despite video's growing popularity (Cisco predicts that by 2021, 82\% of all internet traffic will be video \cite{Cisco}), searching and browsing video remain cumbersome \cite{Kim2014, Pavel2014, Pavel2015}. Video is popular for content creators as it is often easier to author than tutorials or diagrams (which require careful curation). Learners value video for its efficacy in communicating complex or continuous visual actions such as brushing or setting parameters \cite{Chi2012}. However, interacting with videos remains difficult because they are harder to navigate and scan for steps than text \cite{Chi2012}.

%for its popularity as a learning resource \cite{Chi2012, Pongnumkul2011, Nguyen2015}, especially for visual tasks
%its simultaneously the most useful thing for many tasks and also the least well supported by current search interfaces
% while video has become increasingly ubiquitous and easy to create -- ... -- searching and browsing remain cumbersome.

We report on two studies observing how people use RePlay and web video help: a week-long field study ($n\!=\!7$) and a lab study ($n\!=\!24$) where half the participants used RePlay and half used web video search. Both used visual design as the domain, as video is especially helpful for visual tasks. The field study examined how designers with varying experience used RePlay in-situ. Participants used an average of 17 different applications in a week, emphasizing the importance of system-wide integration. It also suggested that contextual video assistance benefits targeted tasks more than open-ended ones. The lab study found that contextual video assistance helps people spend less time away from their task than web video search, and replaces strategies typically used in navigating between and within videos. This work makes the following contributions:

\begin{enumerate}
    \item An application-independent method to find relevant clips in learning videos that leverages user context,
    \item the RePlay system, which demonstrates this method using accessibility \textsc{api}s and online video corpora, and
    \item insights from two studies that highlight the importance of multi-app support and promise of cross-app search.
\end{enumerate}




% replay is an example implementation of such a tool / embodiment of this approach / design probe to illustrate how this could be done, and investigate whether it helps people. / show that this is a viable direction for future research 

% people dream of this thing. we offer a new path to achieve aspects of that goal. knitting help together lowers the friction between applications

% arg 1: lack of integration creates friction that our work mitigates
% ****arg 2: if you need to assemble a buncha tools that all work differnetly you won't know them all
% arg 3: the seams / hand off points require guidance

%Currently, it is difficult for users to get effective guidance when tasks span applications: in-situ help and official documentation is only available at the individual application level, and community-created resources that sometimes do span applications move the user out of their workflow and into the browser. 


%include Google YT timeline marker result as an example
%have IoT example for camera-ready video
% somewhere also mention accessibility for all argument

%watching ppl use 2 different video interfaces taught us what a good video interface should be (wouldn't be able to find this otherwise)
